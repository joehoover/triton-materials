{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is adapted from the Triton [tutorial](https://triton-lang.org/master/getting-started/tutorials/01-vector-add.html) for vector addition. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "size = 98432\n",
    "x = torch.rand(size, device='cuda')\n",
    "y = torch.rand(size, device='cuda')\n",
    "output_torch = x + y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concepts used in `add_kernel`.**\n",
    "\n",
    "* **program.** In Triton, a *program* is more-or-less an instantiation of a kernel that is executed against a specific selection of data. Programs execute the smallest unit of work in the Triton framework. Accordingly, the number of programs required to execute against kernel against a specific set of inputs will depend on the size of the inputs and the size of the program. For example, vector addition for vectors of length 256 with a program block size of 64 would require `256/64=4` programs. And each program would access a different range of elements in the input vectors, i.e. `[0:64, 64:128, 128:192, 192:256]`.\n",
    "\n",
    "* **pointer.** A pointer is a variable that stores the memory address of another variable. For example, assume `x=1` and `x_ptr` is a pointer to `x`. In this case, the value of `x_ptr` is the memory of address of `x`. If you retrieve the value at `x_ptr`, then you have *dereferenced* `x_ptr`.\n",
    "\n",
    "* **DRAM.** Dynamic RAM. The most common but also slowest RAM in GPUs today. GPU code is usually IO bound–a disproportionate amount of time is spent on read/write operations–and optimization often reduces to minimizing DRAM IO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def add_kernel(\n",
    "    x_ptr,\n",
    "    y_ptr,\n",
    "    output_ptr,\n",
    "    n_elements,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # \"\"\"\n",
    "    # A Triton kernel for vector addition. \n",
    "\n",
    "    # Note: Each torch.tensor object is implicitly converted to a pointer to its first element.\n",
    "\n",
    "    # Parameters\n",
    "    # ----------\n",
    "    # x_ptr,\n",
    "    #     A pointer for the first input vector. \n",
    "    # y_ptr,\n",
    "    #     A pointer for the second input vector. \n",
    "    # output_ptr,\n",
    "    #     A pointer for the output vector.\n",
    "    # n_elements,\n",
    "    #     Size of the vector space being added. Vector addition\n",
    "    #     requires uniform dimensions across across vectors, so \n",
    "    #     we only need one representation of vector size.\n",
    "    # BLOCK_SIZE,\n",
    "    #     Number of elements each program should process. `constexpr` is used so that\n",
    "    #     it can be set as a shape value.\n",
    "    # \"\"\"\n",
    "    # There are multiple program's processing different slices of the input data. \n",
    "    # We identify which program we are via the `program_id` method, which will return\n",
    "    # the id of the current program instance along the given axis. Axis, here, refers\n",
    "    # the axis of the 3D launch grid and, accordingly, it must be in [0, 1, 2]. \n",
    "    # As you'll see lat, we use a 1D launch grid, so we specify `axis=0`.\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    # This program will process inputs that are offset from the input data.\n",
    "    # `block_start` specifies where the current block starts. For example, \n",
    "    # for the first block, `pid=0`, so `block_start=0`. But, for the second\n",
    "    # block, `block_start = 1 * BLOCK_SIZE = BLOCK_SIZE`.\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "\n",
    "    # Here, we create a list of pointers that correspond to the offsets\n",
    "    # for this program's slice of the data. We start with the beginning of the block\n",
    "    # and increment through the size of the block. I.e., `offsets` is a list of pointers.\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # Now, we create a mask to guard memory operations against out-of-bounds accesses. \n",
    "    # This will allow us to mask out portions of vector pointers and only load the range specified\n",
    "    # by our offsets.\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    # Load x and y from DRAM, masking out any extra elements in the vectors in case the input is\n",
    "    # not a multiple of the block size. For example, if we have `n_elements=255` and `BLOCK_SIZE=256`,\n",
    "    # we would mask the final position pointer.\n",
    "    # \n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "\n",
    "    # Write x + y back to DRAM, masking any extra elements in case the output vectors are not \n",
    "    # multiples of the block size. Here, if `mask[idx]==False` then `value[idx]` is not \n",
    "    # stored at `pointer[idx]`.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concepts used in `add` function.**\n",
    "\n",
    "* **Launch grid.** In Triton, a launch grid denotes the number of kernel instances that run in parallel. It is analogous to CUDA launch grids. It can be either `Tuple[int]` or `Callable(metaparameters) -> Tuple[int]`. The dimensionality of the launch grid must match the axis specification in your Triton kernel `progam_id` calls. \n",
    "\n",
    "* **`triton.cdiv`.** `triton.cdiv` is [defined](https://github.com/openai/triton/blob/9626c8e944d37f4a62ba2901c90bc7a704111fc1/python/triton/utils.py#L6) as: \n",
    "    ```\n",
    "    def cdiv(x, y):\n",
    "        return (x + y - 1) // y \n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.4645e-11])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(\n",
    "    x: torch.Tensor, \n",
    "    y: torch.Tensor,\n",
    "    block_size: int = 1024\n",
    "):\n",
    "    \"\"\"\n",
    "    A helper function that: \n",
    "        (1) Allocates an output tensor\n",
    "        (2) Enqueues the `add_kernel` with appropriate grid and block sizes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: \n",
    "        A input vector will be added to `y`\n",
    "    y:\n",
    "        An input vector that will be added to `x`\n",
    "    block_size:\n",
    "        Block size that should be used to execute the kernel.\n",
    "    \"\"\"\n",
    "\n",
    "    # Preallocate the output tensor\n",
    "    output = torch.empty_like(x)\n",
    "\n",
    "    # Make sure each tensor is on GPU\n",
    "    assert x.is_cuda and y.is_cuda and output.is_cuda\n",
    "\n",
    "    # Define `n_elemements` as the number of elements in our output tensor.\n",
    "    # Note, this would not be a good idea if our input and output tensors had\n",
    "    # different numbers of elements ;).\n",
    "    n_elements = output.numel()\n",
    "\n",
    "    # Now we specify our launch grid. \n",
    "\n",
    "    # Here, we specify our launch grid. In this case, we use a 1D grid \n",
    "    # where the size is the number of blocks.\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "\n",
    "    # Here, `grid` is a callable that returns a 1D tuple with the required number of blocks (i.e. programs),\n",
    "    # given a specified `n_elements` and `BLOCK_SIZE`. \n",
    "    # For example, if `n_elements=256, meta['BLOCK_SIZE']=256`, then \n",
    "    # `grid(n_elements, meta['BLOCK_SIZE'])==(1,)`. However, \n",
    "    # if `n_elements=512, meta['BLOCK_SIZE']=256`, then \n",
    "    # `grid(n_elements, meta['BLOCK_SIZE'])==(2,)`.\n",
    "\n",
    "\n",
    "    # NOTE:\n",
    "    #  - each torch.tensor object is implicitly converted to a pointer to its first element.\n",
    "    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel\n",
    "    #  - don't forget to pass meta-parameters as keywords arguments\n",
    "\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=block_size)\n",
    "\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1864, 0.6511, 0.2439,  ..., 0.2770, 1.7019, 0.2928], device='cuda:0')\n",
      "tensor([1.1864, 0.6511, 0.2439,  ..., 0.2770, 1.7019, 0.2928], device='cuda:0')\n",
      "The maximum difference between torch and triton is 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "size = 98432\n",
    "x = torch.rand(size, device='cuda')\n",
    "y = torch.rand(size, device='cuda')\n",
    "output_torch = x + y\n",
    "output_triton = add(x, y)\n",
    "print(output_torch)\n",
    "print(output_triton)\n",
    "print(\n",
    "    f'The maximum difference between torch and triton is '\n",
    "    f'{torch.max(torch.abs(output_torch - output_triton))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_triton = add(x, y, block_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_triton = add(x, y, block_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True, False, False, False, False, False])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets = torch.arange(0, 10)\n",
    "offsets < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cdiv in module triton.utils:\n",
      "\n",
      "cdiv(x, y)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(triton.cdiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'cdiv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m help(torch\u001b[39m.\u001b[39;49mcdiv)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'cdiv'"
     ]
    }
   ],
   "source": [
    "help(torch.cdiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "\n",
    "n_elements = 30\n",
    "BLOCK_SIZE = 10\n",
    "meta = {'BLOCK_SIZE': BLOCK_SIZE}\n",
    "\n",
    "grid(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(meta)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_elements = 20\n",
    "BLOCK_SIZE = 10\n",
    "meta = {'BLOCK_SIZE': BLOCK_SIZE}\n",
    "\n",
    "grid(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid({'BLOCK_SIZE':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.666666666666668"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdiv(x,y):\n",
    "    print('x + y: ', x + y)\n",
    "    print('x + y - 1: ', x + y - 1)\n",
    "    print('y: ', y)\n",
    "\n",
    "    return (x + y - 1) // y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y:  13\n",
      "x + y - 1:  12\n",
      "y:  11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdiv(2,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
